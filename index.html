<!DOCTYPE html>
<html>

<head lang="en">


    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous" />
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script
        type="text/javascript">WebFont.load({ google: { families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Changa One:400,400italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500", "Bungee Outline:regular"] } });</script>
    <!--[if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js"></script>
    <script src="assets/js/script.js" type="text/javascript"></script>

    <link href="assets/css/style.css" rel="stylesheet" type="text/css" />

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Cavemen</title>

    <meta name="description"
        content="üóø Cavemen: A prehistoric approach for Mapless Navigation">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <!--FACEBOOK-->
    <meta property="og:image" content="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cae3b53b42ebb3dd4175a82_68747470733a2f2f7777772e69636f6e66696e6465722e636f6d2f646174612f69636f6e732f6f637469636f6e732f313032342f6d61726b2d6769746875622d3235362e706e67.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1280">
    <meta property="og:image:height" content="720">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://gabriel-bronfman.github.io/cavemen/" />
    <meta property="og:title" content="Cavemen" />
    <meta property="og:description"
        content="Project page for üóø Cavemen: A prehistoric approach for Mapless Navigation." />

    <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="üóø Cavemen" />
    <meta name="twitter:description"
        content="Project page for üóø Cavemen: A prehistoric approach for Mapless Navigation." />
    <meta name="twitter:image" content="" />


    <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <link rel="icon"
        href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üóø</text></svg>">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="assets/css/app.css">

    <link rel="stylesheet" href="assets/css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
</head>

<body>
<div class="section">

    <div class="container" id="main">

        <div class="title-row">
            <h1 class="title">Cavemen</h1>
            <h1 class="subheader">üóø A prehistoric approach for Mapless Navigation üó∫Ô∏è</h1>
        </div>
        <div class="base-row author-row">
            <div class="base-col author-col">
                <a href="https://github.com/gabriel-bronfman" target="_blank" class="author-text">
                    Gabriel Bronfman
                </a>
            </div>
            <div class="base-col author-col">
                <a href="https://iamshubhamgupto.github.io" target="_blank" class="author-text">
                    Shubham Gupta
                </a>
            </div>
        </div>
        <div>
            <h1 id="nyu">New York University</h1>
        </div>
        <div class="title-row">
            <h2 class="subheader"></h1>
        </div>
        <div class="link-labels base-row">
            <div class="base-col icon-col"><a href="https://docs.google.com/presentation/d/1VS014bRyqm4rN3s_tJweIRwDdKjDeuX9mA59doqHDfc/edit?usp=sharing" target="_blank"
                    class="link-block"><img
                        src="https://static.vecteezy.com/system/resources/previews/017/396/796/original/google-slides-icon-free-png.png"
                        alt="slides"
                        sizes="(max-width: 479px) 12vw, (max-width: 767px) 7vw, (max-width: 991px) 41.8515625px, 56.6953125px"
                        srcset="https://static.vecteezy.com/system/resources/previews/017/396/796/original/google-slides-icon-free-png.png 500w, https://static.vecteezy.com/system/resources/previews/017/396/796/original/google-slides-icon-free-png.png 672w"
                        class="icon-img" /></a></div>
            <div class="base-col icon-col"><a href="https://github.com/gabriel-bronfman/cavemen" class="link-block"><img
                        src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cae3b53b42ebb3dd4175a82_68747470733a2f2f7777772e69636f6e66696e6465722e636f6d2f646174612f69636f6e732f6f637469636f6e732f313032342f6d61726b2d6769746875622d3235362e706e67.png"
                        alt="code" class="icon-img github-img-icon" /></a></div>
        </div>
        <div class="link-labels base-row">
            <div class="base-col icon-col">
                <strong class="link-labels-text">Slides</strong>
            </div>
            <div class="base-col icon-col">
                <strong class="link-labels-text">&lt;/Code&gt;</strong>
            </div>
        </div>

        <div class="row" style="padding-top: 30px;">
            <image src="assets/gif/teaser.gif" style="width:100%;" class="img-responsive center-block" alt="overview">
            <p class="text-justify">
                <b>(a)</b> The data structure overview of how mapping is done in the exploration phase. <b>(b)</b> The top 12 resulting targets and their corresponding (x,y,w) which is the displacement in the x, y axis and rotation. The bottom four images show the target front, right, back and left view respectively.
            </p><br>
            <h3>
                Abstract
            </h3>
            <p class="text-justify">
                Given a maze without a predefined map, our objective is to navigate to a target location using only visual information. The maze is presented to us in the form of game with two modes: Exploration and Navigation. We use a dead-reckoning approach to estimate the global position of the robot during the Exploration page. We then use a visual bag of words approach to recognize the target location trained using FAISS. We then use A* algorithm to generate a path to the target. The map is presented to the user which is then used to navigate to the target.
            </p>
            <br>
        </div>
        <div class="row">
            <h3>
                Video
            </h3>
            <div class="text-center">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/QXMniZyYJJ0?si=p2gO6OeUsUAsy8AI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </div>
        </div>
        <div class="row">
            <h3>
                Exploration
            </h3>
            <image src="assets/img/vis_nav_final.png" style="width:100%;" class="img-responsive center-block" alt="exploration-img">
                <p class="text-justify">
                    Overview of the system architecture of our solution. The current frame, pose and graph data is published by player.py using <b>Redis</b> which is used by processes running in parallel to update the position of the robot in game in <b>real-time</b> using plotter.py. We visualize the orientation of the robot using arrow.py
                </p><br>
            <p class="paragraph">
                We manually explore the maze using visual inspection and manual control. We use the distance travelled and the amount rotated (rotation limited to 90 degree turns) to create a dead-reckoning approach to calculate the global position of the robot. Index matching each image with each pose, we can create a geotagged list of pictures that we can later interpret for the navigation
            </p>
        </div>
        <div class="row">
            <h3>
                Navigation
            </h3>
            <image src="assets/img/nav-annotate.png" style="width:100%;" class="img-responsive center-block" alt="exploration-img">
                <p class="text-center">
                    Composite view of all windows that allow for fastest navigation to target.
                </p><br>
                <p class="paragraph">
                    We have one method for our recognition: <a href="http://vision.stanford.edu/teaching/cs131_fall1718/files/14_BoW_bayes.pdf">Visual Bag of Words</a>. To ensure accuracy, we train a vocabulary for each run using SIFT features.
                    After training our vocabulary using the <a href="https://faiss.ai">FAISS algorithm</a>, we create histograms of each image we have seen during exploration.
                    We do all of this heavy duty processing during the mapping phase to save time on the navigation phase.
                </p>
                <p class="paragraph">
                    Now, using the index matched data structure described above, we can reliably compare the target images histograms to the total list of histograms we previously created.
                    We shortlist our top 3 candidates for each perspective, and then compare them via visual inspection to the target.
                </p>
                <p class="paragraph">
                    We can manually input the target that we believe is the most accurate, and then the <a href="http://theory.stanford.edu/~amitp/GameProgramming/AStarComparison.html">A* algorithm</a> will generate a path.
                </p>
        </div>

        <div class="row">
            <div class="citation add-top-padding">
                <h3> Citation </h3>
                    <p> If you use this work or find it helpful, please consider citing: (bibtex) </p>
                <pre id="codecell0">@software{Bronfman_Cavemen_A_2023,
    author = {Bronfman, Gabriel and Gupta, Shubham},
    month = dec,
    title = {{üóø Cavemen: A prehistoric approach for Mapless Navigation}},
    url = {https://github.com/gabriel-bronfman/cavemen},
    version = {1.0.0},
    year = {2023}
}</pre>

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-center">
                    The website template is inspired by <a href="https://ai4ce.github.io/SO-NeRF/">SO-NeRF</a>, <a href="https://www.lerf.io/">LERF</a> and <a href="https://creiser.github.io/merf/">MERF</a>. The project is built from the <a href="https://github.com/ai4ce/vis_nav_player">starter code</a> released by Professor Chen Feng and the TAs for the course Robot Perception.
                </p>
            </div>
        </div>

    </div>
</div>
</body>

</html>
